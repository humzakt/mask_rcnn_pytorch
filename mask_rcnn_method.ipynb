{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.ndimage import label\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from dataset import GlaucomaDataset\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, array_to_img, img_to_array\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GlaucomaDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', output_size=(256,256), max_images=None):\n",
    "        self.output_size = output_size\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.images = []\n",
    "        self.segs = []\n",
    "        self.max_images = max_images\n",
    "        # Load data index\n",
    "        for direct in self.root_dir:\n",
    "                    self.image_filenames = []\n",
    "                    for path in os.listdir(os.path.join(direct, \"Images_Square\")):\n",
    "                        if(not path.startswith('.')):\n",
    "                            self.image_filenames.append(path)\n",
    "\n",
    "                    num_images = 0\n",
    "                    for k in range(len(self.image_filenames)):\n",
    "                        # Skip loading if max_images is specified and the limit has been reached\n",
    "                        if max_images is not None and num_images >= max_images:\n",
    "                            break\n",
    "\n",
    "                        print('Loading {} image {}/{}...'.format(split, k, len(self.image_filenames)), end='\\r')\n",
    "                        img_name = os.path.join(direct, \"Images_Square\", self.image_filenames[k])\n",
    "                        img = np.array(Image.open(img_name).convert('RGB'))\n",
    "\n",
    "                        if split != 'test':\n",
    "                            seg_name = os.path.join(direct, \"Masks_Square\", self.image_filenames[k][:-3] + \"png\")\n",
    "                            mask = np.array(Image.open(seg_name, mode='r'))\n",
    "                            od = (mask==1.).astype(np.float32)\n",
    "                            oc = (mask==2.).astype(np.float32)\n",
    "                            \n",
    "                            # Check if both masks are not empty, i.e., they contain at least one non-zero pixel\n",
    "                            if np.any(od) and np.any(oc):\n",
    "                                img = transforms.functional.to_tensor(img)\n",
    "                                img = transforms.functional.resize(img, output_size, interpolation=Image.BILINEAR)\n",
    "                                self.images.append(img)\n",
    "                                od = torch.from_numpy(od[None,:,:])\n",
    "                                oc = torch.from_numpy(oc[None,:,:])\n",
    "                                od = transforms.functional.resize(od, output_size, interpolation=Image.Resampling.NEAREST)\n",
    "                                oc = transforms.functional.resize(oc, output_size, interpolation=Image.Resampling.NEAREST)\n",
    "                                self.segs.append(torch.cat([od, oc], dim=0))\n",
    "                                num_images += 1\n",
    "\n",
    "                    print('Succesfully loaded {} dataset.'.format(split) + ' '*50)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "   \n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load image\n",
    "        img = self.images[idx]\n",
    "        # load segmentation masks (for both optic disk and optic cup)\n",
    "        seg = self.segs[idx]\n",
    "        # For instance segmentation, each mask should be a binary mask of shape (H, W).\n",
    "        # Therefore, we need to split the combined mask into two separate masks.\n",
    "        od_mask, oc_mask = seg[0], seg[1]\n",
    "\n",
    "        # Find bounding boxes around each mask. The bounding box is represented as\n",
    "        # [xmin, ymin, width, height], which is the format expected by Mask R-CNN.\n",
    "        od_bbox = self.mask_to_bbox(od_mask.numpy())\n",
    "        oc_bbox = self.mask_to_bbox(oc_mask.numpy())\n",
    "\n",
    "        # The labels are a tensor of class IDs. In this case, you might want to use\n",
    "        # 1 for optic disk and 2 for optic cup, as you did when creating the masks.\n",
    "        labels = torch.tensor([1, 2], dtype=torch.int64)\n",
    "\n",
    "        # Now, we need to put the masks and bounding boxes into the right format.\n",
    "        # The masks should be a tensor of shape (num_objs, H, W),\n",
    "        # and the bounding boxes should be in a (num_objs, 4) tensor.\n",
    "        masks = torch.stack([od_mask, oc_mask])\n",
    "        boxes = torch.tensor([od_bbox, oc_bbox])\n",
    "\n",
    "        # Pack the bounding boxes and labels into a dictionary\n",
    "        target = {\"boxes\": boxes, \"labels\": labels, \"masks\": masks}\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    @staticmethod\n",
    "    def mask_to_bbox(mask):\n",
    "        # Find the bounding box of a binary mask.\n",
    "        # This method assumes that the input is a binary mask with 0s and 1s.\n",
    "        pos = np.where(mask)\n",
    "        xmin = np.min(pos[1])\n",
    "        xmax = np.max(pos[1])\n",
    "        ymin = np.min(pos[0])\n",
    "        ymax = np.max(pos[0])\n",
    "        return [xmin, ymin, xmax - xmin, ymax - ymin]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # Load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2()\n",
    "\n",
    "    # Get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # Replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # And replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train image 32/650...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kx/1v0nrsws1w125163xg5p1bq80000gn/T/ipykernel_82881/2199420622.py:35: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  img = transforms.functional.resize(img, output_size, interpolation=Image.BILINEAR)\n",
      "/Users/humzakhawar/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully loaded train dataset.                                                  \n",
      "Succesfully loaded train dataset.                                                  \n",
      "Succesfully loaded val dataset.                                                  \n"
     ]
    }
   ],
   "source": [
    "root_dirs = [ \"ORIGA\",\"G1020\"]\n",
    "val_dir = [ \"REFUGE\"]\n",
    "lr = 1e-4\n",
    "batch_size = 8\n",
    "num_workers = 0\n",
    "total_epoch = 1\n",
    "\n",
    "train_set = GlaucomaDataset(root_dirs, \n",
    "                          split='train', max_images=50)\n",
    "\n",
    "val_set = GlaucomaDataset(val_dir, \n",
    "                        split='val', max_images=50)\n",
    "\n",
    "train_loader = DataLoader(train_set, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True, \n",
    "                          num_workers=num_workers,\n",
    "                          pin_memory=True,\n",
    "                         )\n",
    "val_loader = DataLoader(val_set, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=False, \n",
    "                        num_workers=num_workers,\n",
    "                        pin_memory=True,\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "# device = torch.device(\"cuda:0\")\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "# Network\n",
    "model = get_instance_segmentation_model(num_classes=2)\n",
    "\n",
    "# Loss\n",
    "seg_loss = torch.nn.BCELoss(reduction='mean')\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "----------\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected target boxes to be a tensor of shape [N, 4], got torch.Size([8, 2, 4]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/humzakhawar/Desktop/fyp datasets/archive/mask_rcnn_method.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/humzakhawar/Desktop/fyp%20datasets/archive/mask_rcnn_method.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m targets \u001b[39m=\u001b[39m [{k: v\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m  targets\u001b[39m.\u001b[39mitems()}]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/humzakhawar/Desktop/fyp%20datasets/archive/mask_rcnn_method.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/humzakhawar/Desktop/fyp%20datasets/archive/mask_rcnn_method.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m output \u001b[39m=\u001b[39m model(inputs, targets)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/humzakhawar/Desktop/fyp%20datasets/archive/mask_rcnn_method.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(loss \u001b[39mfor\u001b[39;00m loss \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/humzakhawar/Desktop/fyp%20datasets/archive/mask_rcnn_method.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/detection/generalized_rcnn.py:67\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     65\u001b[0m boxes \u001b[39m=\u001b[39m target[\u001b[39m\"\u001b[39m\u001b[39mboxes\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(boxes, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m---> 67\u001b[0m     torch\u001b[39m.\u001b[39;49m_assert(\n\u001b[1;32m     68\u001b[0m         \u001b[39mlen\u001b[39;49m(boxes\u001b[39m.\u001b[39;49mshape) \u001b[39m==\u001b[39;49m \u001b[39m2\u001b[39;49m \u001b[39mand\u001b[39;49;00m boxes\u001b[39m.\u001b[39;49mshape[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39m==\u001b[39;49m \u001b[39m4\u001b[39;49m,\n\u001b[1;32m     69\u001b[0m         \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mExpected target boxes to be a tensor of shape [N, 4], got \u001b[39;49m\u001b[39m{\u001b[39;49;00mboxes\u001b[39m.\u001b[39;49mshape\u001b[39m}\u001b[39;49;00m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     70\u001b[0m     )\n\u001b[1;32m     71\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     torch\u001b[39m.\u001b[39m_assert(\u001b[39mFalse\u001b[39;00m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected target boxes to be of type Tensor, got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(boxes)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/__init__.py:1209\u001b[0m, in \u001b[0;36m_assert\u001b[0;34m(condition, message)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(condition) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mTensor \u001b[39mand\u001b[39;00m has_torch_function((condition,)):\n\u001b[1;32m   1208\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(_assert, (condition,), condition, message)\n\u001b[0;32m-> 1209\u001b[0m \u001b[39massert\u001b[39;00m condition, message\n",
      "\u001b[0;31mAssertionError\u001b[0m: Expected target boxes to be a tensor of shape [N, 4], got torch.Size([8, 2, 4])."
     ]
    }
   ],
   "source": [
    "# Move model to the device\n",
    "model.to(device)\n",
    "\n",
    "# If more than one class is used (background is also considered a class)\n",
    "num_classes = 3 \n",
    "\n",
    "# Initialize the metric trackers\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "# Epoch loop\n",
    "for epoch in range(total_epoch):\n",
    "    model.train()\n",
    "    print(\"Epoch {}/{}\".format(epoch+1, total_epoch))\n",
    "    print('-' * 10)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Train loop\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        # targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        targets = [{k: v.squeeze(0).to(device) for k,v in  targets.items()}]\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(inputs, targets)\n",
    "        loss = sum(loss for loss in output.values())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch+1, \n",
    "                                                                       i * len(inputs), \n",
    "                                                                       len(train_loader.dataset),\n",
    "                                                                       100. * i / len(train_loader),\n",
    "                                                                       loss.item()))\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    train_loss.append(epoch_loss)\n",
    "    print('Training Loss: {:.4f}'.format(epoch_loss))\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in enumerate(val_loader):\n",
    "            inputs = list(img.to(device) for img in inputs)\n",
    "            targets = [{k: v.to(device) for k, v in t.items} for t in targets]\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(inputs, targets)\n",
    "            loss = sum(loss for loss in output.values())\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            print('Validate: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(i * len(inputs), \n",
    "                                                                     len(val_loader.dataset),\n",
    "                                                                     100. * i / len(val_loader),\n",
    "                                                                     loss.item()))\n",
    "        epoch_loss = running_loss / len(val_loader.dataset)\n",
    "        val_loss.append(epoch_loss)\n",
    "        print('Validation Loss: {:.4f}'.format(epoch_loss))\n",
    "    print()\n",
    "\n",
    "# Save the trained model\n",
    "# torch.save(model.state_dict(), 'model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 20, 125,  28,  35],\n",
      "         [ 32, 136,  10,  14]],\n",
      "\n",
      "        [[ 75, 111,  33,  35],\n",
      "         [ 81, 117,  22,  22]],\n",
      "\n",
      "        [[ 82, 111,  37,  38],\n",
      "         [ 94, 121,  17,  19]],\n",
      "\n",
      "        [[209, 109,  33,  35],\n",
      "         [219, 117,  14,  17]],\n",
      "\n",
      "        [[121, 108,  26,  28],\n",
      "         [126, 114,  16,  16]],\n",
      "\n",
      "        [[193,  98,  33,  36],\n",
      "         [203, 109,  14,  14]],\n",
      "\n",
      "        [[167, 106,  37,  39],\n",
      "         [173, 113,  22,  24]],\n",
      "\n",
      "        [[ 56,  99,  35,  39],\n",
      "         [ 67, 107,  19,  19]]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "for i, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        # targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        targets = [{k: v.squeeze(1).to(device) for k,v in  targets.items()}]\n",
    "\n",
    "        for target in targets:\n",
    "            print(target[\"boxes\"])\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
